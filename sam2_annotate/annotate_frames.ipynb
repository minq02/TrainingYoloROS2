{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e80584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dcfba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e39081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba44eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f92a7",
   "metadata": {},
   "source": [
    "### View First Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e99eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "video_dir = \"/home/minq02/curly/sam2/CUSTOM/frames\"\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)\\\n",
    "\n",
    "# inference_state = predictor.init_state(\n",
    "#     video_path=video_dir,\n",
    "#     offload_video_to_cpu=True,\n",
    "#     async_loading_frames=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48050b",
   "metadata": {},
   "source": [
    "### Segmenting using box prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c17171",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 0  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "box = np.array([1100, 1310, 2200, 1730], dtype=np.float32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    box=box,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_box(box, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 0  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (460, 60) to refine the mask\n",
    "points = np.array([[1900, 1500], [1200, 1300]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 0], np.int32)\n",
    "# note that we also need to send the original box input along with\n",
    "# the new refinement click together into `add_new_points_or_box`\n",
    "box = np.array([1100, 1310, 2200, 1730], dtype=np.float32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    "    box=box,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_box(box, plt.gca())\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67609d07",
   "metadata": {},
   "source": [
    "### VISUALIZE TRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e65476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 10\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61655c58",
   "metadata": {},
   "source": [
    "### SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/home/minq02/curly/sam2/CUSTOM\"\n",
    "img_dir, mask_dir, ovl_dir, lbl_dir = [os.path.join(base, p) for p in\n",
    "    (\"images\",\"annotated_masks\",\"overlays\",\"labels\")]\n",
    "for d in (img_dir, mask_dir, ovl_dir, lbl_dir): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# naming + stride\n",
    "START_INDEX = 805  # CHANGE\n",
    "PAD = 5\n",
    "IMG_EXT = \".png\"\n",
    "SAVE_STRIDE = 1  # CHANGE\n",
    "\n",
    "alpha = 0.35\n",
    "CLASS_ID = 0\n",
    "MIN_AREA_PX = 100\n",
    "MAX_PTS = 200\n",
    "\n",
    "def to_hw_u8(mask, H, W):\n",
    "    m = np.asarray(mask)\n",
    "    m = (m > 0).astype(np.uint8)\n",
    "    m = np.squeeze(m)\n",
    "    if m.ndim == 1:\n",
    "        assert m.size == H*W\n",
    "        m = m.reshape(H, W)\n",
    "    elif m.ndim == 3:\n",
    "        if m.shape[0] == 1:   m = m[0]\n",
    "        elif m.shape[-1] == 1: m = m[..., 0]\n",
    "        else:                  m = (m > 0).any(axis=-1).astype(np.uint8)\n",
    "    return (m * 255).astype(np.uint8)\n",
    "\n",
    "cur_idx = START_INDEX\n",
    "\n",
    "for frame_idx in range(0, len(frame_names), SAVE_STRIDE):   # <- stride here\n",
    "    stem_new = f\"{cur_idx:0{PAD}d}\"                         # increment only when saved\n",
    "    cur_idx += 1\n",
    "\n",
    "    img_path = os.path.join(video_dir, frame_names[frame_idx])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_np = np.array(img); H, W = img_np.shape[:2]\n",
    "\n",
    "    obj_dict = video_segments.get(frame_idx, {})\n",
    "    masks = list(obj_dict.values())\n",
    "\n",
    "    Image.fromarray(img_np).save(os.path.join(img_dir, f\"{stem_new}{IMG_EXT}\"), quality=95)\n",
    "\n",
    "    if not masks:\n",
    "        open(os.path.join(lbl_dir, f\"{stem_new}.txt\"), \"w\").close()\n",
    "        Image.fromarray(img_np).save(os.path.join(ovl_dir, f\"{stem_new}.png\"))\n",
    "        continue\n",
    "\n",
    "    if len(masks) > 1:\n",
    "        def area(m): return int(np.count_nonzero(to_hw_u8(m, H, W)))\n",
    "        mask = masks[int(np.argmax([area(m) for m in masks]))]\n",
    "    else:\n",
    "        mask = masks[0]\n",
    "\n",
    "    mask_u8 = to_hw_u8(mask, H, W)\n",
    "    Image.fromarray(mask_u8, mode=\"L\").save(os.path.join(mask_dir, f\"{stem_new}.png\"))\n",
    "\n",
    "    contours, _ = cv2.findContours(mask_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    lines = []\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) < MIN_AREA_PX: continue\n",
    "        eps = 0.001 * cv2.arcLength(cnt, True)\n",
    "        cnt = cv2.approxPolyDP(cnt, eps, True).reshape(-1, 2)\n",
    "        if len(cnt) > MAX_PTS:\n",
    "            cnt = cnt[::int(np.ceil(len(cnt)/MAX_PTS))]\n",
    "        if len(cnt) < 3: continue\n",
    "        cnt = cnt.astype(np.float32)\n",
    "        cnt[:,0] = np.clip(cnt[:,0]/W, 0, 1); cnt[:,1] = np.clip(cnt[:,1]/H, 0, 1)\n",
    "        lines.append(f\"{CLASS_ID} \" + \" \".join(f\"{x:.6f} {y:.6f}\" for x,y in cnt))\n",
    "\n",
    "    with open(os.path.join(lbl_dir, f\"{stem_new}.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    overlay = img_np.copy()\n",
    "    if len(contours) > 0:\n",
    "        fill = np.zeros_like(overlay)\n",
    "        cv2.fillPoly(fill, contours, (40, 220, 240))\n",
    "        overlay = cv2.addWeighted(overlay, 1.0, fill, alpha, 0.0)\n",
    "        cv2.drawContours(overlay, contours, -1, (255, 255, 255), 2)\n",
    "    Image.fromarray(overlay).save(os.path.join(ovl_dir, f\"{stem_new}.png\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
